import json
import re
import os
import time
import streamlit as st
import requests
from huggingface_hub import InferenceClient

# ğŸ“Œ Hugging Face API í† í° ê°€ì ¸ì˜¤ê¸° (`secrets.toml`ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
def get_huggingface_token():
    try:
        return st.secrets["HUGGINGFACE_API_TOKEN"]
    except KeyError:
        st.warning("âŒ Hugging Face API í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.streamlit/secrets.toml` íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

# ğŸ“Œ Google Gemma-2-9B-IT ëª¨ë¸ API í˜¸ì¶œ (3íšŒ ì¬ì‹œë„)
def generate_text_via_api(prompt: str, model_name: str = "google/gemma-2-9b-it"):
    """Hugging Face APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    token = get_huggingface_token()
    if token is None:
        return "âŒ API í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.streamlit/secrets.toml` íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."

    client = InferenceClient(model=model_name, api_key=token)

    max_retries = 3  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
    for attempt in range(max_retries):
        try:
            response = client.text_generation(prompt=prompt)
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                st.warning(f"ğŸ”„ Hugging Face ì„œë²„ ì‘ë‹µ ì—†ìŒ. {attempt+1}/{max_retries}íšŒ ì¬ì‹œë„ ì¤‘...")
            else:
                return "âŒ í˜„ì¬ Hugging Face ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."

# ğŸ“Œ ì‚¬ìš©ì ì…ë ¥ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°)
def clean_input(text: str) -> str:
    return re.sub(r"\b(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ì¶”ì²œí•´ ì¤˜|ë§í•´ ì¤˜)\b", "", text, flags=re.IGNORECASE).strip()

# ğŸ“Œ ìë™ì°¨ ê¸°ë³¸ ì •ë³´ ë° ê°€ê²©/ì˜µì…˜ ë¹„êµ ê¸°ëŠ¥
def get_car_info_based_on_question(user_input: str) -> str:
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìë™ì°¨ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
    clean_text = clean_input(user_input)
    prompt = f"""
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìë™ì°¨ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    - ì‚¬ìš©ì ì§ˆë¬¸: "{clean_text}"
    - ìë™ì°¨ì˜ êµ¬ë™ ë°©ì‹(4ë¥œ, ì „ë¥œ, í›„ë¥œ)ì— ëŒ€í•œ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”.
    - ì°¨ëŸ‰ ê¸ˆì•¡ ë° ì˜µì…˜ë³„ ê°€ê²© ì°¨ì´ë¥¼ ì •í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
    - íŠ¹ì • ì°¨ëŸ‰ê³¼ ë‹¤ë¥¸ ì°¨ëŸ‰ ê°„ ì°¨ì´ì ë„ ì„¤ëª…í•˜ì„¸ìš”.

    ì˜ˆì‹œ:
    - ì§ˆë¬¸: "4ë¥œì´ë‘ ì „ë¥œì´ë‚˜ í›„ë¥œì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"
    - ë‹µë³€:
      1. **ì „ë¥œêµ¬ë™(FWD)**: ì•ë°”í€´ê°€ ë™ë ¥ì„ ì „ë‹¬ë°›ì•„ ì›€ì§ì´ëŠ” ë°©ì‹. ì—°ë¹„ê°€ ì¢‹ê³ , ëˆˆê¸¸ì´ë‚˜ ë¹—ê¸¸ì—ì„œ ì•ˆì •ì .
      2. **í›„ë¥œêµ¬ë™(RWD)**: ë’·ë°”í€´ê°€ ë™ë ¥ì„ ë°›ì•„ ì£¼í–‰. ìŠ¤í¬ì¸ ì¹´ë‚˜ ê³ ê¸‰ ì„¸ë‹¨ì— ë§ì´ ì‚¬ìš©ë˜ë©°, ê³ ì† ì£¼í–‰ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨.
      3. **ì‚¬ë¥œêµ¬ë™(AWD, 4WD)**: ë„¤ ë°”í€´ê°€ ëª¨ë‘ ë™ë ¥ì„ ë°›ì•„ ì£¼í–‰. ì˜¤í”„ë¡œë“œ ì£¼í–‰ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë©°, ëˆˆê¸¸ì´ë‚˜ ì‚°ê¸¸ì—ì„œ ì•ˆì •ì .

    - ì§ˆë¬¸: "í˜„ëŒ€ ì•„ì´ì˜¤ë‹‰ 6 ì˜µì…˜ë³„ ê¸ˆì•¡ ì°¨ì´ ì•Œë ¤ì¤˜"
    - ë‹µë³€:
      1. **ìŠ¤íƒ ë‹¤ë“œ íŠ¸ë¦¼**: 4,800ë§Œì›, ì£¼í–‰ê±°ë¦¬ 400km, ê¸°ë³¸ í¸ì˜ ê¸°ëŠ¥ ì œê³µ
      2. **í”„ë¦¬ë¯¸ì—„ íŠ¸ë¦¼**: 5,300ë§Œì›, ì£¼í–‰ê±°ë¦¬ 450km, ê³ ê¸‰ ë‚´ì¥ì¬ ë° ì²¨ë‹¨ ë³´ì¡° ì‹œìŠ¤í…œ í¬í•¨
      3. **ìµœê³ ê¸‰ íŠ¸ë¦¼(í’€ì˜µì…˜)**: 5,900ë§Œì›, ì£¼í–‰ê±°ë¦¬ 500km, ììœ¨ì£¼í–‰ ê¸°ëŠ¥ ë° ìµœê³ ê¸‰ ì‚¬ì–‘ í¬í•¨
    """
    return generate_text_via_api(prompt)

# ğŸš€ Streamlit UI
st.title("ğŸš— AI ìë™ì°¨ ì •ë³´ ì‹œìŠ¤í…œ")

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
st.subheader("ğŸ“‹ ìë™ì°¨ ê´€ë ¨ ì§ˆë¬¸ ì…ë ¥")
user_question = st.text_area("ğŸš€ ì›í•˜ëŠ” ìë™ì°¨ ê´€ë ¨ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”!", placeholder="ì˜ˆ: 4ë¥œì´ë‘ ì „ë¥œ/í›„ë¥œì˜ ì°¨ì´ì ì€?")

# ğŸš€ ê²€ìƒ‰ ë²„íŠ¼
if st.button("ê²€ìƒ‰í•˜ê¸°"):
    if user_question.strip() == "":
        st.warning("â— ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    else:
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            time.sleep(3)
            st.success("âœ… ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

        response = get_car_info_based_on_question(user_question)
        st.subheader("ğŸ” ìë™ì°¨ ì •ë³´")
        st.write(response)